# Agente de Transcripci√≥n de Archivos Multimedia
Caso de Uso ‚Äî Proyecto de Grado: Implementaci√≥n de agentes de IA con LLMs locales en entornos seguros

## üß© Descripci√≥n General

Este repositorio implementa un agente de transcripci√≥n autom√°tica de archivos multimedia (audio o video), dise√±ado para ejecutarse en entornos locales y seguros, sin depender de servicios en la nube.
El flujo es orquestado mediante n8n, utiliza Whisper (modelo de reconocimiento autom√°tico de voz ‚Äì ASR) ejecutado en Ollama, y permite extraer, transcribir y almacenar el texto resultante.

El agente forma parte del proyecto de investigaci√≥n sobre implementaci√≥n de agentes de IA locales con LLMs, aplicado a procesos acad√©mico-administrativos de la universidad.

## üèóÔ∏è Componentes Principales
Componente	Funci√≥n	                                                                              Tecnolog√≠a
n8n	        Orquestaci√≥n de flujo del agente. Define la l√≥gica de carga, procesamiento y salida.	n8n.io

Ollama	    Ejecuci√≥n local de modelos LLM/ASR.	                                                  Ollama

Whisper	    Modelo de reconocimiento autom√°tico de voz (ASR) para transcripci√≥n.	                OpenAI Whisper

Storage	    Carpeta local o base de datos para guardar resultados.	                              Local/SQLite/PostgreSQL


## üß† Flujo de Trabajo (Workflow en n8n)

1. Carga del archivo multimedia

  - El usuario sube un archivo .mp3, .mp4, .wav o .m4a al flujo de n8n.

  - El archivo se almacena temporalmente en una carpeta local (p. ej. /data/input).

2. Llamado al modelo Whisper (ASR)

  - n8n ejecuta una llamada HTTP a Ollama:
    
  ```bash
  POST http://localhost:11434/api/generate
{
  "model": "whisper",
  "input": "@ruta_del_archivo"
}
```

- Ollama procesa el audio y devuelve la transcripci√≥n en texto plano.

3. Procesamiento del texto

  - n8n limpia o segmenta el texto seg√∫n la configuraci√≥n del flujo.

  - Opcional: se puede ejecutar un modelo LLM (por ejemplo, Mistral) para resumir o categorizar la transcripci√≥n.

4. Almacenamiento y salida

  - La transcripci√≥n se guarda en una carpeta local (/data/output/transcripciones).

  - Opcionalmente se puede:

    - Enviar por correo electr√≥nico.

    - Guardar en una base de datos (SQLite, PostgreSQL).

    - Integrar con otro agente (por ejemplo, uno de an√°lisis o b√∫squeda sem√°ntica).

üß∞ Requisitos Previos

- Docker y Docker Compose instalados.

- Al menos 8 GB de RAM (recomendado 16 GB para procesar videos largos).

- Modelos descargados localmente:
```bash
ollama pull whisper
```

## üöÄ Ejecuci√≥n

1. Clonar el repositorio:
```bash
git clone https://github.com/<usuario>/agent-transcripcion-local.git
cd agent-transcripcion-local
```

2. Iniciar los servicios:
```bash
docker compose up -d
```

3. Acceder a n8n:
```bash
http://localhost:5678
```

4. Crear el flujo (workflow):

  - Agregar nodo File Upload / HTTP Request (para recibir el archivo).

  - Agregar nodo HTTP Request para llamar a Ollama Whisper.

  - Agregar nodo Write Binary File / Database / Email para guardar o enviar la transcripci√≥n.

## üß™ Ejemplo de llamada a Whisper
```bash
curl -X POST http://localhost:11434/api/generate \
     -H "Content-Type: application/json" \
     -d '{
           "model": "whisper",
           "input": "@/data/input/clase1.mp3"
         }'
```

Salida esperada:

{
  "response": "Bienvenidos a la clase de hoy sobre inteligencia artificial aplicada a la educaci√≥n..."
}

## üß± Directorios del Proyecto
```bash
agent-transcripcion-local/
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ transcriber/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ input/
‚îÇ   ‚îî‚îÄ‚îÄ output/
‚îú‚îÄ‚îÄ n8n_data/
‚îú‚îÄ‚îÄ ollama_data/
‚îî‚îÄ‚îÄ README.md
```
## üîê Consideraciones de Privacidad y Seguridad

- Todos los procesos se ejecutan localmente, sin enviar datos a servidores externos.

- Los archivos de audio/video no abandonan el entorno institucional.

- Las credenciales y configuraciones se almacenan en variables de entorno seguras.

## üìö Referencias T√©cnicas

n8n Documentation

Ollama Whisper

OpenAI Whisper GitHub

Docker Compose


## ‚öôÔ∏è Arquitectura del Agente

```mermaid
flowchart LR
    A["Archivo multimedia: MP4 / MP3 / WAV"] --> B["n8n - Flujo de orquestaci√≥n"]
    B --> C["Transcriber Service (Flask API)"]
    C --> D["Ollama - Modelo Whisper"]
    D --> E["Texto transcrito"]
    E --> F["Almacenamiento local o base de datos"]
